#!/usr/bin/env python3
"""Generate an inventory of external domains referenced in the repository."""
from __future__ import annotations

import argparse
import json
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Set, Tuple
from urllib.parse import urlparse

URL_RE = re.compile(r"https?://[^\s\"'<>)+]+")
IGNORED_DIRS = {
    ".git",
    "__pycache__",
    "node_modules",
    "out",
    "dist",
    "build",
    "venv",
}
TEXT_FILE_EXTENSIONS = {
    ".py",
    ".js",
    ".ts",
    ".tsx",
    ".jsx",
    ".json",
    ".md",
    ".txt",
    ".html",
    ".css",
    ".yaml",
    ".yml",
    ".toml",
    ".ini",
    ".cfg",
    ".sh",
    ".ps1",
    ".mjs",
    ".svg",
}


def iter_files(root: Path) -> Iterable[Path]:
    for dirpath, dirnames, filenames in os.walk(root):
        dirnames[:] = [d for d in dirnames if d not in IGNORED_DIRS]
        for filename in filenames:
            path = Path(dirpath, filename)
            if should_scan(path):
                yield path


def should_scan(path: Path) -> bool:
    if path.suffix in TEXT_FILE_EXTENSIONS:
        return True
    # Fallback: treat small files without null bytes as text.
    try:
        with path.open("rb") as fh:
            chunk = fh.read(1024)
        if b"\x00" in chunk:
            return False
        return True
    except OSError:
        return False


def extract_urls(path: Path) -> Iterable[str]:
    try:
        text = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        try:
            text = path.read_text(encoding="latin-1")
        except Exception:
            return []
    return URL_RE.findall(text)


def build_inventory(root: Path) -> Tuple[Dict[str, Set[Path]], Dict[str, Set[str]]]:
    domain_to_files: Dict[str, Set[Path]] = defaultdict(set)
    domain_to_urls: Dict[str, Set[str]] = defaultdict(set)
    for file_path in iter_files(root):
        for url in extract_urls(file_path):
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            if not domain:
                continue
            domain_to_files[domain].add(file_path.relative_to(root))
            domain_to_urls[domain].add(url)
    return domain_to_files, domain_to_urls


def render_markdown(
    root: Path, domain_to_files: Dict[str, Set[Path]], domain_to_urls: Dict[str, Set[str]]
) -> str:
    domains = sorted(domain_to_files.keys())
    lines: List[str] = []
    lines.append("# External Domain Inventory\n")
    lines.append(
        f"This document lists {len(domains)} unique external domains referenced in this repository.\n"
    )
    lines.append("Generated by `tools/domain_inventory.py`.\n")

    for domain in domains:
        lines.append(f"\n## {domain}\n")
        lines.append("**Referenced URLs**\n")
        for url in sorted(domain_to_urls[domain]):
            lines.append(f"- {url}")
        lines.append("\n**Files**\n")
        for file_path in sorted(domain_to_files[domain]):
            lines.append(f"- `{file_path}`")
    lines.append("")
    return "\n".join(lines)


def main() -> None:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--root",
        type=Path,
        default=Path.cwd(),
        help="Repository root to scan.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Optional output file path. Defaults to stdout if not provided.",
    )
    parser.add_argument(
        "--json",
        type=Path,
        default=None,
        help="Optional JSON output capturing the full mapping.",
    )
    args = parser.parse_args()

    root = args.root.resolve()
    domain_to_files, domain_to_urls = build_inventory(root)
    markdown = render_markdown(root, domain_to_files, domain_to_urls)

    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        args.output.write_text(markdown, encoding="utf-8")
    else:
        print(markdown)

    if args.json:
        serialisable = {
            domain: {
                "urls": sorted(map(str, domain_to_urls[domain])),
                "files": sorted(str(path) for path in domain_to_files[domain]),
            }
            for domain in sorted(domain_to_files)
        }
        args.json.parent.mkdir(parents=True, exist_ok=True)
        args.json.write_text(json.dumps(serialisable, indent=2), encoding="utf-8")


if __name__ == "__main__":
    main()
